<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Accumulo™</title>
    <description>The Apache Accumulo™ sorted, distributed key/value store is a robust, scalable, high performance data storage and retrieval system.
</description>
    <link>https://accumulo.apache.org/</link>
    <atom:link href="https://accumulo.apache.org/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 05 May 2021 17:10:04 +0000</pubDate>
    <lastBuildDate>Wed, 05 May 2021 17:10:04 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
    
      <item>
        <title>Apache Accumulo 2.0.1</title>
        <description>&lt;p&gt;Apache Accumulo 2.0.1 contains bug fixes for 2.0.0.&lt;/p&gt;

&lt;p&gt;Since 2.0 is a non-LTM release line, and since an LTM release line has not yet
been made available for 2.x, this patch backports critical bug fixes to 2.0 to
address security bug &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt; that could affect any existing 2.0.0
users. Users that have already migrated to 2.0.0 are urged to upgrade to 2.0.1
as soon as possible, and users of 1.10 who wish to upgrade to 2.0 should
upgrade directly to 2.0.1, bypassing 2.0.0.&lt;/p&gt;

&lt;p&gt;These release notes are highlights of the changes since 2.0.0. The full
detailed changes can be seen in the git history. If anything is missing from
this list, please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us to have it included.&lt;/p&gt;

&lt;h2 id=&quot;critical-bug-fixes&quot;&gt;Critical Bug Fixes&lt;/h2&gt;

&lt;p&gt;This release includes critical bug fixes to fix security bugs identified as
&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1828&quot;&gt;#1828&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1832&quot;&gt;#1832&lt;/a&gt; Throw exceptions when permission checks fail,
and improve test coverage for permissions checks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-bug-fixes&quot;&gt;Other Bug Fixes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1839&quot;&gt;#1839&lt;/a&gt; Fix AccumuloClient’s builder to prevent it from modifying a
provided Properties object when building a client from Properties&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;note-about-newer-jdk-versions-11-and-later&quot;&gt;Note About Newer JDK Versions (11 and later)&lt;/h2&gt;

&lt;p&gt;While work has been done on other branches to better support newer JDK
versions, that is not the case for this non-LTM release. Certain non-critical
aspects of this release are known to break with some newer versions of JDK.&lt;/p&gt;

&lt;p&gt;For example, the version of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maven-javadoc-plugin&lt;/code&gt; may fail to generate the
javadocs using a newer JDK’s javadoc tool. In addition, this release assumes
the use of the CMS garbage collector in its build tests and in minicluster.
Newer JDKs, where CMS has been removed, may fail to execute Accumulo build
tests in this release.&lt;/p&gt;

&lt;p&gt;Therefore, it is recommended to use JDK 8 or JDK 11 with this release, which
are known to work.&lt;/p&gt;

&lt;h2 id=&quot;note-about-zookeeper-versions-35-and-later&quot;&gt;Note About ZooKeeper Versions 3.5 and Later&lt;/h2&gt;

&lt;p&gt;This release assumes the use of ZooKeeper 3.4. While work has been done on
other branches to better support newer ZooKeeper versions (3.5 and later), this
is a targeted release to fix specific bugs and does not include those kinds of
improvements.&lt;/p&gt;

&lt;p&gt;Therefore, in order to use this release with ZooKeeper versions 3.5 and later,
you may need to edit your default class path, or perform other minor changes to
work smoothly with those versions of ZooKeeper. Please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us if you need
assistance working with newer versions of ZooKeeper.&lt;/p&gt;

&lt;h2 id=&quot;upgrading&quot;&gt;Upgrading&lt;/h2&gt;

&lt;p&gt;View the &lt;a href=&quot;/docs/2.x/administration/upgrading&quot;&gt;Upgrading Accumulo documentation&lt;/a&gt; for guidance.&lt;/p&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/r0835b67240060cae394c7e4a7ad18a7238f17cabc7a508aa176c95c9%40%3Cdev.accumulo.apache.org%3E&quot;&gt;Release VOTE email thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/compare/rel/2.0.0...apache:rel/2.0.1&quot;&gt;All Changes since 2.0.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues?q=project%3Aapache%2Faccumulo%2F19&quot;&gt;GitHub&lt;/a&gt; - List of issues tracked on GitHub corresponding to this release&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/release/accumulo-2.0.1/</link>
        <guid isPermaLink="true">https://accumulo.apache.org/release/accumulo-2.0.1/</guid>
        
        
        <category>release</category>
        
      </item>
    
      <item>
        <title>Apache Accumulo 1.10.1</title>
        <description>&lt;h2 id=&quot;about&quot;&gt;About&lt;/h2&gt;

&lt;p&gt;Apache Accumulo 1.10.1 is a bug fix release of the 1.10 LTM release line.&lt;/p&gt;

&lt;p&gt;These release notes are highlights of the changes since 1.10.0. The full
detailed changes can be seen in the git history. If anything is missing from
this list, please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us to have it included.&lt;/p&gt;

&lt;p&gt;Users of 1.10.0 or earlier are urged to upgrade to 1.10.1 as soon as possible,
as this is a continuation of the 1.10 LTM release line with critical bug fixes
for security bug &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt;. Users are also encouraged to consider
migrating to a 2.x version when one that is suitable for their needs becomes
available.&lt;/p&gt;

&lt;h2 id=&quot;critical-bug-fixes&quot;&gt;Critical Bug Fixes&lt;/h2&gt;

&lt;p&gt;This release includes critical bug fixes to fix security bugs identified as
&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1830&quot;&gt;#1830&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1832&quot;&gt;#1832&lt;/a&gt; Throw exceptions when permission checks fail,
and improve test coverage for permissions checks (backport of &lt;a href=&quot;https://github.com/apache/accumulo/issues/1828&quot;&gt;#1828&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-bug-fixes&quot;&gt;Other Bug Fixes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1716&quot;&gt;#1716&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1729&quot;&gt;#1729&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1737&quot;&gt;#1737&lt;/a&gt; Improvements in tool.sh,
including better support for newer ZooKeeper and Hadoop versions&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1829&quot;&gt;#1829&lt;/a&gt; Improve log message in Delete Cleanup FATE&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1734&quot;&gt;#1734&lt;/a&gt; Support building native libraries on alpine-based distros&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;note-about-jdk-15&quot;&gt;Note About JDK 15&lt;/h2&gt;

&lt;p&gt;Accumulo 1.x assumes the use of the CMS garbage collector in its build tests
and in the minicluster code. That garbage collector was removed in newer
versions of Java, and the build flags for Java that supported configuring the
CMS garbage collector now cause errors if attempted to be used with Java 15 or
later.&lt;/p&gt;

&lt;p&gt;Therefore, a change was made in 1.10.1’s build to fail fast if attempting to
build with JDK 15 or later (using JDK 11 or later was already a build
requirement).&lt;/p&gt;

&lt;p&gt;If you need to build on JDK 15 or later, and intend to skip tests and don’t
intend to use minicluster, you can bypass this build constraint by building
with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Denforcer.skip&lt;/code&gt;, as a workaround.&lt;/p&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/r90ac3cc0d2356c86a94abf2b6859965e9659b8bcdb6cfd18b69941ac%40%3Cdev.accumulo.apache.org%3E&quot;&gt;Release VOTE email thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/compare/rel/1.10.0...apache:rel/1.10.1&quot;&gt;All Changes since 1.10.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues?q=project%3Aapache%2Faccumulo%2F16&quot;&gt;GitHub&lt;/a&gt; - List of issues tracked on GitHub corresponding to this release&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/release/accumulo-1.10.1/</link>
        <guid isPermaLink="true">https://accumulo.apache.org/release/accumulo-1.10.1/</guid>
        
        
        <category>release</category>
        
      </item>
    
      <item>
        <title>Apache Accumulo 1.10.0</title>
        <description>&lt;h2 id=&quot;about&quot;&gt;About&lt;/h2&gt;

&lt;p&gt;Apache Accumulo 1.10.0 is a continuation of the 1.x release line, and is
essentially the next maintenance release of 1.8/1.9, following the 1.9.3
version with some small additional internal improvements. Earlier 1.x versions
are now superseded by this maintenance release, and will no longer be
maintained.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://semver.org/spec/v2.0.0.html&quot;&gt;semver&lt;/a&gt; minor version number increase (1.9 to 1.10) signals that this
release is backwards compatible with previous minor releases (1.8 and 1.9).
Rather than API additions, the primary reason for this minor version increase
is due to the decision to make Java 8 the minimum supported Java version (see
below for more).&lt;/p&gt;

&lt;p&gt;This release contains contributions from more than 13 contributors from the
Apace Accumulo community in over 80 commits and 16 months of work since the
1.9.3 release. These release notes are highlights of those changes. The full
detailed changes can be seen in the git history. If anything is missing from
this list, please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us to have it included.&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&quot;/contributor/versioning#LTM&quot;&gt;Long Term Maintenance (LTM)&lt;/a&gt; strategy, the intent is to
maintain the 1.10 release line with critical bug and security fixes until one
year after the next LTM version is released. However, this is anticipated to be
the final 1.x legacy release, so it is not expected to receive any new features
or significant non-critical updates, so users wanting new features should plan
to upgrade to a 2.x release, where new feature development is still being done.&lt;/p&gt;

&lt;p&gt;Users of 1.9.3 or earlier are urged to upgrade to 1.10.0 as soon as it is
available, as this is a continuation of the 1.9 maintenance line. and to
consider migrating to a 2.x version when a suitable one becomes available.
Accumulo 2.0.0 is currently available, and 2.1.0 is anticipated to be the next
LTM release. If you would like to start preparing for 2.1.0 now, one way to do
this is to start building and testing the next version of your software against
Accumulo 2.0.0 because all 2.x releases should be backwards compatible with
2.0.0, following &lt;a href=&quot;https://semver.org/spec/v2.0.0.html&quot;&gt;semantic versioning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;minimum-requirements&quot;&gt;Minimum Requirements&lt;/h2&gt;

&lt;p&gt;The versions mentioned here are a guide only. It is not expected that our
convenience binary tarball will work out-of-the-box with your particular
environment, and some responsibility is placed on users to properly configure
Accumulo, or even patch and rebuild it from source, for their particular
environment.&lt;/p&gt;

&lt;p&gt;Please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us or file a &lt;a href=&quot;https://github.com/apache/accumulo/issues&quot;&gt;bug report&lt;/a&gt; if you have trouble with a
specific version or wish to seek tips. Be prepared to provide details of the
problems you encounter, as well as perform some troubleshooting steps of your
own, in order to get the best response.&lt;/p&gt;

&lt;h3 id=&quot;java-8&quot;&gt;Java 8&lt;/h3&gt;

&lt;p&gt;Java 8 is now the minimum supported Java version, and it is designed to work on
Java 11, as well. To build the project from source, Java 11 or later is
required. Please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us if you find any bugs on any Java version.&lt;/p&gt;

&lt;h3 id=&quot;hadoop-2-or-3&quot;&gt;Hadoop 2 or 3&lt;/h3&gt;

&lt;p&gt;This release has been built using Hadoop 2.6.5, and is expected to work with
any Hadoop version 2.6.5 or later. It has also been tested with 3.0.3, and is
expected to work with Hadoop 3.0 versions as well. Hadoop 3.1.3, 3.2.1, and
3.3.0 have also been tested with this version, and are known to work (with at
least basic functionality) with some class path modifications (specifically,
using Guava 27.0-jre instead of the provided 14.0 version).&lt;/p&gt;

&lt;p&gt;Particular class path pain points are known to be guava, commons-io,
commons-vfs2, and possibly other commons libraries.&lt;/p&gt;

&lt;h3 id=&quot;zookeeper&quot;&gt;ZooKeeper&lt;/h3&gt;

&lt;p&gt;This release has been built agains ZooKeeper 3.4.14, the latest 3.4 release. It
is known to work against 3.5 and 3.6 versions as well, when configured
properly.&lt;/p&gt;

&lt;h2 id=&quot;major-bug-fixes&quot;&gt;Major Bug Fixes&lt;/h2&gt;

&lt;h3 id=&quot;accumulo-gc-bug&quot;&gt;Accumulo GC Bug&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1314&quot;&gt;#1314&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1318&quot;&gt;#1318&lt;/a&gt; Eliminate task creation leak caused by the an
additional timed-task created for each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-gc&lt;/code&gt; cycle&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bulk-import-concurrency-bug&quot;&gt;Bulk Import Concurrency Bug&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1153&quot;&gt;#1153&lt;/a&gt; Prevent multiple threads from working on same bulk file&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prevent-metadata-corruption&quot;&gt;Prevent Metadata Corruption&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1309&quot;&gt;#1309&lt;/a&gt; Prevent cloning of the metadata table, which could lead to
data loss during &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-gc&lt;/code&gt; for either the clone or the original
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.metadata&lt;/code&gt; table&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1310&quot;&gt;#1310&lt;/a&gt; Improve GC handling of WALs used by root tablet. If the root
tablet had WALs, the GC did not consider them during collection&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1379&quot;&gt;#1379&lt;/a&gt; During GC scans, an error will be thrown if the GC fails
consistency checks; added a check to ensure the last tablet was seen&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-miscellaneous-bug-fixes&quot;&gt;Other Miscellaneous Bug Fixes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1107&quot;&gt;#1107&lt;/a&gt; Fix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ConcurrentModificationException&lt;/code&gt; in
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HostRegexTableLoadBalancer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1185&quot;&gt;#1185&lt;/a&gt; Fixed a bug where we were using an unauthenticated ZooKeeper
client to try to read data with an ACL configured; this was previously
permitted until ZooKeeper fixed a security bug in their own code, which
revealed our incorrect ZooKeeper client code&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1371&quot;&gt;#1371&lt;/a&gt; Fix a bug in our MapReduce code that prevented some users from
reading tables they had valid permissions to read&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1401&quot;&gt;#1401&lt;/a&gt; Display trace information correctly in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-monitor&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1478&quot;&gt;#1478&lt;/a&gt; Don’t ignore the instance and zookeepers parameters on the
command-line when running certain utilities&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1532&quot;&gt;#1532&lt;/a&gt; Remove need for ANT on classpath&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1555&quot;&gt;#1555&lt;/a&gt; Fix idempotency bug in importtable&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1644&quot;&gt;#1644&lt;/a&gt; Retry minor compactions to prevent transient iterator issues
blocking forever&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;major-improvements&quot;&gt;Major Improvements&lt;/h2&gt;

&lt;h3 id=&quot;performance-enhancements&quot;&gt;Performance Enhancements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/990&quot;&gt;#990&lt;/a&gt; Avoid multiple threads loading same cache block&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1352&quot;&gt;#1352&lt;/a&gt; Add an option to configure the metadata action after an
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-gc&lt;/code&gt; cycle using a new property instead of a hard-coded compaction
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_gc_post_metadata_action&quot;&gt;gc.post.metadata.action&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1462&quot;&gt;#1462&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1526&quot;&gt;#1526&lt;/a&gt; Temporarily cache the existence check for
recovery WALs, so multiple tablets pointing to the same WAL to avoid
expensive redundant checks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;identifying-busy-tablets&quot;&gt;Identifying Busy Tablets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1291&quot;&gt;#1291&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1296&quot;&gt;#1296&lt;/a&gt; Log busy tablets by ingest and query at
configurable intervals for better hot-spot detection using new properties
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_tserver_log_busy_tablets_count&quot;&gt;tserver.log.busy.tablets.count&lt;/a&gt; and&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_tserver_log_busy_tablets_interval&quot;&gt;tserver.log.busy.tablets.interval&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tserver-startup-and-shutdown-protections&quot;&gt;TServer Startup and Shutdown Protections&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1158&quot;&gt;#1158&lt;/a&gt; Require a configurable number of servers to be online, up to a
max wait time, before assignments begin on startup
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_master_startup_tserver_avail_min_count&quot;&gt;master.startup.tserver.avail.min.count&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_master_startup_tserver_avail_max_wait&quot;&gt;master.startup.tserver.avail.max.wait&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1456&quot;&gt;#1456&lt;/a&gt; Throttle the number of shutdown requests sent to tservers to
prevent cluster self-destruction and give time for triage&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;new-metrics&quot;&gt;New Metrics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1406&quot;&gt;#1406&lt;/a&gt; Add GC cycle metrics (file and wal collection) to be reported
via the hadoop2 metrics. This exposes the gc cycle metrics available in the
monitor to external metrics systems and includes run time for the new gc post
operation (compact, flush)
    &lt;ul&gt;
      &lt;li&gt;Enable with new property, &lt;a href=&quot;/1.10/accumulo_user_manual#_gc_metrics_enabled&quot;&gt;gc.metrics.enabled&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcCandidates&lt;/code&gt; - number of candidates for GC&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcDeleted&lt;/code&gt; - number of candidates deleted&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcErrors&lt;/code&gt; - number of deletion errors&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcFinished&lt;/code&gt; - timestamp of GC cycle finished&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcInUse&lt;/code&gt; - number of candidates still in use&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcPostOpDuration&lt;/code&gt; - duration of compact / flush&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcRunCycleCount&lt;/code&gt; - 1-up cycle count&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcStarted&lt;/code&gt; - timestamp of GC cycle start&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalCandidates&lt;/code&gt; - number of WAL candidates for collection&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalDeleted&lt;/code&gt; - number of WALs deleted&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalErrors&lt;/code&gt; - number of errors during WAL deletion&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalFinished&lt;/code&gt; - timestamp of WAL collection completion&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalInUse&lt;/code&gt; - number of WALs in use&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalStarted&lt;/code&gt; - timestamp of WAL collection start&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-miscellaneous-improvements&quot;&gt;Other Miscellaneous Improvements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1108&quot;&gt;#1108&lt;/a&gt; Improve logging when ZooKeeper session expires&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1299&quot;&gt;#1299&lt;/a&gt; Add optional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-t&lt;/code&gt; tablename to importdirectory shell command&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1338&quot;&gt;#1338&lt;/a&gt; Reduce verbose logging of merge operations in Master log&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1475&quot;&gt;#1475&lt;/a&gt; Option to leave cloned tables offline on creation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1503&quot;&gt;#1503&lt;/a&gt; Support ZooKeeper 3.5 (and later), in addition to 3.4&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/rd4731d4fd87c30958ad82a8b0be9375f2562ab0a9531ea037e646f3c%40%3Cdev.accumulo.apache.org%3E&quot;&gt;Release VOTE email thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/compare/rel/1.9.3...apache:rel/1.10.0&quot;&gt;All Changes since 1.9.3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues?q=project%3Aapache%2Faccumulo%2F8&quot;&gt;GitHub&lt;/a&gt; - List of issues tracked on GitHub corresponding to this release&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/release/accumulo-1.10.0/</link>
        <guid isPermaLink="true">https://accumulo.apache.org/release/accumulo-1.10.0/</guid>
        
        
        <category>release</category>
        
      </item>
    
      <item>
        <title>Microsoft MASC, an Apache Spark connector for Apache Accumulo</title>
        <description>&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/masc&quot;&gt;MASC&lt;/a&gt; provides an Apache Spark native connector for Apache Accumulo to integrate the rich Spark machine learning eco-system with the scalable and secure data storage capabilities of Accumulo.&lt;/p&gt;

&lt;h2 id=&quot;major-features&quot;&gt;Major Features&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Simplified Spark DataFrame read/write to Accumulo using DataSource v2 API&lt;/li&gt;
  &lt;li&gt;Speedup of 2-5x over existing approaches for pulling key-value data into DataFrame format&lt;/li&gt;
  &lt;li&gt;Scala and Python support without overhead for moving between languages&lt;/li&gt;
  &lt;li&gt;Process streaming data from Accumulo without loading it all into Spark memory&lt;/li&gt;
  &lt;li&gt;Push down filtering with a flexible expression language (&lt;a href=&quot;http://juel.sourceforge.net/&quot;&gt;JUEL&lt;/a&gt;): user can define logical operators and comparisons to reduce the amount of data returned from Accumulo&lt;/li&gt;
  &lt;li&gt;Column pruning based on selected fields transparently reduces the amount of data returned from Accumulo&lt;/li&gt;
  &lt;li&gt;Server side inference: ML model inference can run on the Accumulo nodes using MLeap to increase the scalability of AI solutions as well as keeping data in Accumulo&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use-cases&lt;/h2&gt;
&lt;p&gt;MASC is advantageous in many use-cases, below we list a few.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 1&lt;/strong&gt;: A data analyst needs to execute model inference on large amount of data in Accumulo.&lt;br /&gt;
&lt;strong&gt;Benefit&lt;/strong&gt;: Instead of transferring all the data to a large Spark cluster to score using a Spark model, the connector exports and runs the model on the Accumulo cluster. This reduces the need for a large Spark cluster as well as the amount of data transferred between systems, and can improve inference speeds (&amp;gt;2x speedups observed).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 2&lt;/strong&gt;: A data scientist needs to train a Spark model on a large amount of data in Accumulo.&lt;br /&gt;
&lt;strong&gt;Benefit&lt;/strong&gt;: Instead of pulling all the data into a large Spark cluster and restructuring the format to use Spark ML Lib tools, the connector streams data into Spark as a DataFrame reducing time to train and Spark cluster size / memory requirements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 3&lt;/strong&gt;: A data analyst needs to perform ad hoc analysis on large amounts of data stored in Accumulo.&lt;br /&gt;
&lt;strong&gt;Benefit&lt;/strong&gt;: Instead of pulling all the data into a large Spark cluster, the connector prunes rows and columns using pushdown filtering with a flexible expression language.&lt;/p&gt;

&lt;h1 id=&quot;architecture&quot;&gt;Architecture&lt;/h1&gt;
&lt;p&gt;The Accumulo-Spark connector is composed of two components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Accumulo server-side iterator performs
    &lt;ul&gt;
      &lt;li&gt;column pruning&lt;/li&gt;
      &lt;li&gt;row-based filtering&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/combust/mleap&quot;&gt;MLeap&lt;/a&gt; ML model inference and&lt;/li&gt;
      &lt;li&gt;row assembly using &lt;a href=&quot;https://avro.apache.org/&quot;&gt;Apache AVRO&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Spark DataSource V2
    &lt;ul&gt;
      &lt;li&gt;determines the number of Spark tasks based on available Accumulo table splits&lt;/li&gt;
      &lt;li&gt;translates Spark filter conditions into a &lt;a href=&quot;http://juel.sourceforge.net/&quot;&gt;JUEL&lt;/a&gt; expression&lt;/li&gt;
      &lt;li&gt;configures the Accumulo iterator&lt;/li&gt;
      &lt;li&gt;deserializes the AVRO payload&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202002_masc/architecture.svg&quot; alt=&quot;Architecture&quot; title=&quot;MASC Architecture Diagram&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;usage&quot;&gt;Usage&lt;/h1&gt;
&lt;p&gt;More detailed documentation on installation and use is available in the 
&lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/README.md&quot;&gt;Connector documentation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Java 8&lt;/li&gt;
  &lt;li&gt;Spark 2.4.3+&lt;/li&gt;
  &lt;li&gt;Accumulo 2.0.0+&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JARs available on Maven Central Repository:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://search.maven.org/search?q=g:%22com.microsoft.masc%22%20AND%20a:%22microsoft-accumulo-spark-datasource%22&quot;&gt;&lt;img src=&quot;https://img.shields.io/maven-central/v/com.microsoft.masc/microsoft-accumulo-spark-datasource.svg?label=Maven%20Central&quot; alt=&quot;Maven Central&quot; /&gt; Spark DataSource&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://search.maven.org/search?q=g:%22com.microsoft.masc%22%20AND%20a:%22microsoft-accumulo-spark-iterator%22&quot;&gt;&lt;img src=&quot;https://img.shields.io/maven-central/v/com.microsoft.masc/microsoft-accumulo-spark-iterator.svg?label=Maven%20Central&quot; alt=&quot;Maven Central&quot; /&gt; Accumulo Iterator&lt;/a&gt; - Backend for Spark DataSource&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;example-use&quot;&gt;Example use&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;configparser&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConfigParser&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark.sql&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;types&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Read Accumulo client properties file&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConfigParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[top]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'top'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/opt/muchos/install/accumulo-2.0.0/conf/accumulo-client.properties'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'table'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'demo_table'&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Define Accumulo table where data will be written
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rowkey'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Identify column to use as the key for Accumulo rows
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# define the schema
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentiment&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IntegerType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;date&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;query_string&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Read from Accumulo
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;com.microsoft.accumulo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# define Accumulo properties
&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;# define schema for data retrieval
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Write to Accumulo
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'table'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'output_table'&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;com.microsoft.accumulo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See the &lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnector.ipynb&quot;&gt;demo notebook&lt;/a&gt; for more examples.&lt;/p&gt;

&lt;h1 id=&quot;computational-performance-of-ai-scenario&quot;&gt;Computational Performance of AI Scenario&lt;/h1&gt;
&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;
&lt;p&gt;The benchmark setup used a 1,000-node Accumulo 2.0.0 Cluster (16,000 cores) running and a 256-node Spark 2.4.3 cluster (4,096 cores). All nodes used &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general&quot;&gt;Azure D16s_v3&lt;/a&gt; (16 cores) virtual machines. &lt;a href=&quot;https://github.com/apache/fluo-muchos&quot;&gt;Fluo-muchos&lt;/a&gt; was used to handle Accumulo and Spark cluster deployments and configuration.&lt;/p&gt;

&lt;p&gt;In all experiments we use the same base dataset which is a collection of Twitter user tweets with labeled sentiment value. This dataset is known as the Sentiment140 dataset (&lt;a href=&quot;https://www-nlp.stanford.edu/courses/cs224n/2009/fp/3.pdf&quot;&gt;Go, Bhayani, &amp;amp; Huang, 2009&lt;/a&gt;). The training data consist of 1.6M samples of tweets, where each tweet has columns indicating the sentiment label, user, timestamp, query term, and text. The text is limited to 140 characters and the overall uncompressed size of the training dataset is 227MB.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;sentiment&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;query_string&lt;/th&gt;
      &lt;th&gt;user&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1467810369&lt;/td&gt;
      &lt;td&gt;Mon Apr 06 22:19:…&lt;/td&gt;
      &lt;td&gt;NO_QUERY&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;TheSpecialOne&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;@switchfoot http:…&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1467810672&lt;/td&gt;
      &lt;td&gt;Mon Apr 06 22:19:…&lt;/td&gt;
      &lt;td&gt;NO_QUERY&lt;/td&gt;
      &lt;td&gt;scotthamilton&lt;/td&gt;
      &lt;td&gt;is upset that he …&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1467810917&lt;/td&gt;
      &lt;td&gt;Mon Apr 06 22:19:…&lt;/td&gt;
      &lt;td&gt;NO_QUERY&lt;/td&gt;
      &lt;td&gt;mattycus&lt;/td&gt;
      &lt;td&gt;@Kenichan I dived…&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To evaluate different table sizes and the impact of splitting the following procedure was used to generate the Accumulo tables:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prefix id with split keys (e.g. 0000, 0001, …, 1024)&lt;/li&gt;
  &lt;li&gt;Create Accumulo table and configure splits&lt;/li&gt;
  &lt;li&gt;Upload prefixed data to Accumulo using Spark and the MASC writer&lt;/li&gt;
  &lt;li&gt;Duplicate data using custom Accumulo server-side iterator&lt;/li&gt;
  &lt;li&gt;Validate data partitioning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A common machine learning scenario was evaluated using a sentiment model trained using &lt;a href=&quot;https://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;SparkML&lt;/a&gt;. 
To train the classification model, we generated feature vectors from the text of tweets (text column). We used a feature engineering pipeline (a.k.a. featurizer) that breaks the text into tokens, splitting on whitespaces and discarding any capitalization and non-alphabetical characters. The pipeline consisted of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regex Tokenizer&lt;/li&gt;
  &lt;li&gt;Hashing Transformer&lt;/li&gt;
  &lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the &lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnectorBenchmark.ipynb&quot;&gt;benchmark notebook (Scala)&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The first set of experiments evaluated data transfer efficiency and ML model inference performance. The chart below shows&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Accumulo table split size (1GB, 8GB, 32GB, 64GB)&lt;/li&gt;
  &lt;li&gt;Total table size (1TB, 10TB, 100TB, 1PB)&lt;/li&gt;
  &lt;li&gt;Operations
    &lt;ul&gt;
      &lt;li&gt;Count: plain count of the data&lt;/li&gt;
      &lt;li&gt;Inference: Accumulo server-side inference using MLeap&lt;/li&gt;
      &lt;li&gt;Transfer: Filtering results for 30% data transfer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Time is reported in minutes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Remarks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Time is log-scale&lt;/li&gt;
  &lt;li&gt;Inference was run with and without data transfer to isolate server-side performance.&lt;/li&gt;
  &lt;li&gt;The smaller each Accumulo table split is, the more splits we have and thus higher parallelization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202002_masc/runtime.png&quot; alt=&quot;Runtime&quot; title=&quot;Runtime Performance&quot; class=&quot;blog-img-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second set of experiments highlights the computational performance improvement of using the server-side inference approach compared to running inference on the Spark cluster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202002_masc/sparkml_vs_mleap_accumulo.png&quot; alt=&quot;Mleap&quot; title=&quot;Spark ML vs MLeap Performance&quot; class=&quot;blog-img-center&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;learnings&quot;&gt;Learnings&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Accumulo MLeap Server-side inference vs Spark ML results in a 2x improvement&lt;/li&gt;
  &lt;li&gt;Multi-threading in Spark jobs can be used to fully utilize Accumulo servers
    &lt;ul&gt;
      &lt;li&gt;Useful when Spark cluster has less cores than Accumulo&lt;/li&gt;
      &lt;li&gt;e.g. 8 threads * 2,048 Spark executor = 16,384 Accumulo threads&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unbalanced Accumulo table splits can introduce performance bottlenecks&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;useful-links&quot;&gt;Useful links&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnector.ipynb&quot;&gt;Complete Jupyter demo notebook (PySpark)&lt;/a&gt; for usage of the Accumulo-Spark connector&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnectorBenchmark.ipynb&quot;&gt;Complete Jupyter benchmark notebook (Scala)&lt;/a&gt; for usage of the Accumulo-Spark connector&lt;/li&gt;
  &lt;li&gt;GitHub Repository &lt;a href=&quot;https://github.com/microsoft/masc&quot;&gt;Microsoft’s contributions for Spark with Apache Accumulo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/combust/mleap&quot;&gt;MLeap&lt;/a&gt; - Scala/Java stand-alone model inference for SparkML-based models&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;SparkML&lt;/a&gt; - Spark machine learning library&lt;/li&gt;
  &lt;li&gt;MASC Maven artifacts
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://mvnrepository.com/artifact/com.microsoft.masc/microsoft-accumulo-spark-iterator&quot;&gt;Accumulo Iterator - Backend for Spark DataSource&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://mvnrepository.com/artifact/com.microsoft.masc/microsoft-accumulo-spark-datasource&quot;&gt;Spark DataSource&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;license&quot;&gt;License&lt;/h1&gt;
&lt;p&gt;This work is publicly available under the Apache License 2.0 on GitHub under &lt;a href=&quot;https://github.com/microsoft/masc&quot;&gt;Microsoft’s contributions for Apache Spark with Apache Accumulo&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;contributions&quot;&gt;Contributions&lt;/h1&gt;
&lt;p&gt;Feedback, questions, and contributions are welcome!&lt;/p&gt;

&lt;p&gt;Thanks to contributions from members on the Azure Global Customer Engineering and Azure Government teams.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/AnupamMicrosoft&quot;&gt;Anupam Sharma&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/arvindshmicrosoft&quot;&gt;Arvind Shyamsundar&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/billierinaldi&quot;&gt;Billie Rinaldi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chenhuims&quot;&gt;Chenhui Hu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/loomlike&quot;&gt;Jun-Ki Min&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/phrocker&quot;&gt;Marc Parisi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/eisber&quot;&gt;Markus Cozowicz&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Pavandeep Kalra&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/roalexan&quot;&gt;Robert Alexander&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gramhagen&quot;&gt;Scott Graham&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wutaomsft&quot;&gt;Tao Wu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Special thanks to &lt;a href=&quot;https://github.com/ancasarb&quot;&gt;Anca Sarb&lt;/a&gt; for promptly assisting with &lt;a href=&quot;https://github.com/combust/mleap/issues/633&quot;&gt;MLeap performance issues&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2020/02/26/accumulo-spark-connector.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2020/02/26/accumulo-spark-connector.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Accumulo Clients in Other Programming Languages</title>
        <description>&lt;p&gt;Apache Accumulo has an &lt;a href=&quot;https://github.com/apache/accumulo-proxy&quot;&gt;Accumulo Proxy&lt;/a&gt; that allows communication with Accumulo using clients written
in languages other than Java. This blog post shows how to run the Accumulo Proxy process using &lt;a href=&quot;https://github.com/apache/fluo-uno&quot;&gt;Uno&lt;/a&gt;
and communicate with Accumulo using a Python client.&lt;/p&gt;

&lt;p&gt;First, clone the &lt;a href=&quot;https://github.com/apache/accumulo-proxy&quot;&gt;Accumulo Proxy&lt;/a&gt; repository.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/apache/accumulo-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Assuming you have &lt;a href=&quot;https://github.com/apache/fluo-uno&quot;&gt;Uno&lt;/a&gt; set up on your machine, configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uno.conf&lt;/code&gt; to start the &lt;a href=&quot;https://github.com/apache/accumulo-proxy&quot;&gt;Accumulo Proxy&lt;/a&gt;
by setting the configuration below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export POST_RUN_PLUGINS=&quot;accumulo-proxy&quot;
export PROXY_REPO=/path/to/accumulo-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run the following command to set up Accumulo again. The Proxy will be started after Accumulo runs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;uno setup accumulo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After Accumulo is set up, you should see the following output from uno:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Executing post run plugin: accumulo-proxy
Installing Accumulo Proxy at /path/to/fluo-uno/install/accumulo-proxy-2.0.0-SNAPSHOT
Accumulo Proxy 2.0.0-SNAPSHOT is running
    * view logs at /path/to/fluo-uno/install/logs/accumulo-proxy/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, follow the instructions below to create a Python 2.7 client that creates an Accumulo table
named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pythontest&lt;/code&gt; and writes data to it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir accumulo-client/
cd accumulo-client/
pipenv --python 2.7
pipenv install thrift
pipenv install -e /path/to/accumulo-proxy/src/main/python
cp /path/to/accumulo-proxy/src/main/python/basic_client.py .
# Edit credentials if needed
vim basic_client.py
pipenv run python2 basic_client.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify that the table was created or data was written using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uno ashell&lt;/code&gt; or the Accumulo monitor.&lt;/p&gt;

</description>
        <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/12/16/accumulo-proxy.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/12/16/accumulo-proxy.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Checking API use</title>
        <description>&lt;p&gt;Accumulo follows &lt;a href=&quot;https://semver.org/&quot;&gt;SemVer&lt;/a&gt; across versions with the declaration of a public API.  Code not in the public API should be
considered unstable, at risk of changing between versions.  The public API packages are &lt;a href=&quot;/api/&quot;&gt;listed on the website&lt;/a&gt;
but may not be considered when an Accumulo user writes code.  This blog post explains how to make Maven
automatically detect usage of Accumulo code outside the public API.&lt;/p&gt;

&lt;p&gt;The techniques described in this blog post only work for Accumulo 2.0 and later.  Do not use with 1.X versions.&lt;/p&gt;

&lt;h2 id=&quot;checkstyle-plugin&quot;&gt;Checkstyle Plugin&lt;/h2&gt;

&lt;p&gt;First add the checkstyle Maven plugin to your pom.&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;plugin&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- This was added to ensure project only uses Accumulo's public API --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.maven.plugins&lt;span class=&quot;nt&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;maven-checkstyle-plugin&lt;span class=&quot;nt&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;3.1.0&lt;span class=&quot;nt&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;executions&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;execution&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;id&amp;gt;&lt;/span&gt;check-style&lt;span class=&quot;nt&quot;&gt;&amp;lt;/id&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;goals&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;nt&quot;&gt;&amp;lt;goal&amp;gt;&lt;/span&gt;check&lt;span class=&quot;nt&quot;&gt;&amp;lt;/goal&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;/goals&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;nt&quot;&gt;&amp;lt;configLocation&amp;gt;&lt;/span&gt;checkstyle.xml&lt;span class=&quot;nt&quot;&gt;&amp;lt;/configLocation&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;/execution&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/executions&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/plugin&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The plugin version is the latest at the time of this post.  For more information see the website for
the &lt;a href=&quot;https://maven.apache.org/plugins/maven-checkstyle-plugin/&quot;&gt;Apache Maven Checkstyle Plugin&lt;/a&gt;.  The configuration above adds the plugin to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt; execution goal
so it will always run with your build.&lt;/p&gt;

&lt;p&gt;Create the configuration file specified above: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkstyle.xml&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;checkstylexml&quot;&gt;checkstyle.xml&lt;/h3&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE module PUBLIC &quot;-//Puppy Crawl//DTD Check Configuration 1.3//EN&quot; &quot;http://www.puppycrawl.com/dtds/configuration_1_3.dtd&quot;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;module&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Checker&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;property&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;charset&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;module&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TreeWalker&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!--check that only Accumulo public APIs are imported--&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;module&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ImportControl&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;property&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;file&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;import-control.xml&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/module&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/module&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/module&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This file sets up the ImportControl module.&lt;/p&gt;

&lt;h2 id=&quot;import-control-configuration&quot;&gt;Import Control Configuration&lt;/h2&gt;

&lt;p&gt;Create the second file specified above, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import-control.xml&lt;/code&gt; and copy the configuration below.  Make sure to replace
“insert-your-package-name” with the package name of your project.&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE import-control PUBLIC
    &quot;-//Checkstyle//DTD ImportControl Configuration 1.4//EN&quot;
    &quot;https://checkstyle.org/dtds/import_control_1_4.dtd&quot;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;&amp;lt;!-- This checkstyle rule is configured to ensure only use of Accumulo API --&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;import-control&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;insert-your-package-name&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;strategyOnMismatch=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;allowed&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- API packages --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.client&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.data&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.security&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.iterators&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.minicluster&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.hadoop.mapreduce&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- disallow everything else coming from accumulo --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;disallow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/import-control&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This file configures the ImportControl module to only allow packages that are declared public API.&lt;/p&gt;

&lt;h2 id=&quot;hold-the-line&quot;&gt;Hold the line&lt;/h2&gt;

&lt;p&gt;Adding this to an existing project may expose usage of non public Accumulo API’s. It may take more time than is available
to fix those at first, but do not let this discourage adding this plugin. One possible way to proceed is to allow the
currently used non-public APIs in a commented section of import-control.xml noting these are temporarily allowed until
they can be removed. This strategy prevents new usages of non-public APIs while allowing time to work on fixing the current
 usages of non public APIs.  Also, if you don’t want your project failing to build because of this, you can add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;failOnViolation&amp;gt;false&amp;lt;/failOnViolation&amp;gt;&lt;/code&gt;
to the maven-checkstyle-plugin configuration.&lt;/p&gt;

</description>
        <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/11/04/checkstyle-import-control.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/11/04/checkstyle-import-control.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Using Azure Data Lake Gen2 storage as a data store for Accumulo</title>
        <description>&lt;p&gt;Accumulo can store its files in &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction&quot;&gt;Azure Data Lake Storage Gen2&lt;/a&gt;
using the &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-abfs-driver&quot;&gt;ABFS (Azure Blob File System)&lt;/a&gt; driver.
Similar to &lt;a href=&quot;/blog/2019/09/10/accumulo-S3-notes.html&quot;&gt;S3 blog&lt;/a&gt;,
the write ahead logs &amp;amp; Accumulo metadata can be stored in HDFS and everything else on Gen2 storage
using the volume chooser feature introduced in Accumulo 2.0. The configurations referred on this blog
are specific to Accumulo 2.0 and Hadoop 3.2.0.&lt;/p&gt;

&lt;h2 id=&quot;hadoop-setup&quot;&gt;Hadoop setup&lt;/h2&gt;

&lt;p&gt;For ABFS client to talk to Gen2 storage, it requires one of the Authentication mechanism listed &lt;a href=&quot;https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html#Authentication&quot;&gt;here&lt;/a&gt;
This post covers &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&quot;&gt;Azure Managed Identity&lt;/a&gt;
formerly known as Managed Service Identity or MSI. This feature provides Azure services with an 
automatically managed identity in &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-whatis&quot;&gt;Azure AD&lt;/a&gt;
and it avoids the need for credentials or other sensitive information from being stored in code 
or configs/JCEKS. Plus, it comes free with Azure AD.&lt;/p&gt;

&lt;p&gt;At least the following should be added to Hadoop’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;core-site.xml&lt;/code&gt; on each node.&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.auth.type&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;OAuth&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.oauth.provider.type&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.oauth2.msi.tenant&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;TenantID&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.oauth2.client.id&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;ClientID&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See &lt;a href=&quot;https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html&quot;&gt;ABFS doc&lt;/a&gt;
for more information on Hadoop Azure support.&lt;/p&gt;

&lt;p&gt;To get hadoop command to work with ADLS Gen2 set the 
following entries in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hadoop-env.sh&lt;/code&gt;. As Gen2 storage is TLS enabled by default, 
it is important we use the native OpenSSL implementation of TLS.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTIONAL_TOOLS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hadoop-azure&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-Dorg.wildfly.openssl.path=&amp;lt;path/to/OpenSSL/libraries&amp;gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To verify the location of the OpenSSL libraries, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;whereis libssl&lt;/code&gt; command 
on the host&lt;/p&gt;

&lt;h2 id=&quot;accumulo-setup&quot;&gt;Accumulo setup&lt;/h2&gt;

&lt;p&gt;For each node in the cluster, modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-env.sh&lt;/code&gt; to add Azure storage jars to the
classpath.  Your versions may differ depending on your Hadoop version,
following versions were included with Hadoop 3.2.0.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_CONF_DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ZOOKEEPER_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/client/*&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/azure-data-lake-store-sdk-2.2.9.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/azure-keyvault-core-1.0.0.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/hadoop-azure-3.2.0.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/wildfly-openssl-1.0.4.Final.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jaxb-api-2.2.11.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/commons-lang3-3.7.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/httpclient-4.5.2.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;CLASSPATH
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Dorg.wildfly.openssl.path&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JAVA_OPTS&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-env.sh&lt;/code&gt; as shown below. This
java property is an optional performance enhancement for TLS.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;JAVA_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACCUMULO_JAVA_OPTS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[@]&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:+UseConcMarkSweepGC'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:CMSInitiatingOccupancyFraction=75'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:+CMSClassUnloadingEnabled'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:OnOutOfMemoryError=kill -9 %p'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:-OmitStackTraceInFastThrow'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-Djava.net.preferIPv4Stack=true'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-Dorg.wildfly.openssl.path=/usr/lib64'&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;-Daccumulo.native.lib.path=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/native&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set the following in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt; and then run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo init&lt;/code&gt;, but don’t start Accumulo.&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;py&quot;&gt;instance.volumes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;name node&amp;gt;/accumulo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After running Accumulo init we need to configure storing write ahead logs in
HDFS.  Set the following in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;py&quot;&gt;instance.volumes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;namenode&amp;gt;/accumulo,abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulo&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.volume.chooser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;org.apache.accumulo.server.fs.PreferredVolumeChooser&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.custom.volume.preferred.default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulo&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.custom.volume.preferred.logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;namenode&amp;gt;/accumulo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo init --add-volumes&lt;/code&gt; to initialize the Azure DLS Gen2 volume.  Doing this
in two steps avoids putting any Accumulo metadata files in Gen2  during init.
Copy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt; to all nodes and start Accumulo.&lt;/p&gt;

&lt;p&gt;Individual tables can be configured to store their files in HDFS by setting the
table property &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;table.custom.volume.preferred&lt;/code&gt;.  This should be set for the
metadata table in case it splits using the following Accumulo shell command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config -t accumulo.metadata -s table.custom.volume.preferred=hdfs://&amp;lt;namenode&amp;gt;/accumulo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;accumulo-example&quot;&gt;Accumulo example&lt;/h2&gt;

&lt;p&gt;The following Accumulo shell session shows an example of writing data to Gen2 and
reading it back.  It also shows scanning the metadata table to verify the data
is stored in Gen2.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@muchos&amp;gt; createtable gen2test
root@muchos gen2test&amp;gt; insert r1 f1 q1 v1
root@muchos gen2test&amp;gt; insert r1 f1 q2 v2
root@muchos gen2test&amp;gt; flush -w
2019-10-16 08:01:00,564 [shell.Shell] INFO : Flush of table gen2test  completed.
root@muchos gen2test&amp;gt; scan
r1 f1:q1 []    v1
r1 f1:q2 []    v2
root@muchos gen2test&amp;gt; scan -t accumulo.metadata -c file
4&amp;lt; file:abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulo/tables/4/default_tablet/F00000gj.rf []    234,2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These instructions will help to configure Accumulo to use Azure’s Data Lake Gen2 Storage along with HDFS. With this setup, 
we are able to successfully run the continuos ingest test. Going forward, we’ll experiment more on this space 
with ADLS Gen2 and add/update blog as we come along.&lt;/p&gt;

</description>
        <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/10/15/accumulo-adlsgen2-notes.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/10/15/accumulo-adlsgen2-notes.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Using HDFS Erasure Coding with Accumulo</title>
        <description>&lt;p&gt;HDFS normally stores multiple copies of each file for both performance and durability reasons. 
The number of copies is controlled via HDFS replication settings, and by default is set to 3. Hadoop 3, 
introduced the use of erasure coding (EC), which improves durability while decreasing overhead.
Since Accumulo 2.0 now supports Hadoop 3, it’s time to take a look at whether using
EC with Accumulo makes sense.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#ec-intro&quot;&gt;EC Intro&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ec-performance&quot;&gt;EC Performance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#accumulo-performance-with-ec&quot;&gt;Accumulo Performance with EC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ec-intro&quot;&gt;EC Intro&lt;/h3&gt;

&lt;p&gt;By default HDFS achieves durability via block replication.  Usually
the replication count is 3, resulting in a storage overhead of 200%. Hadoop 3 
introduced EC as a better way to achieve durability.  More info can be
found &lt;a href=&quot;https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html&quot;&gt;here&lt;/a&gt;.
EC behaves much like RAID 5 or 6…for &lt;em&gt;k&lt;/em&gt; blocks of data, &lt;em&gt;m&lt;/em&gt; blocks of
parity data are generated, from which the original data can be recovered in the
event of disk or node failures (erasures, in EC parlance).  A typical EC scheme is Reed-Solomon 6-3, where
6 data blocks produce 3 parity blocks, an overhead of only 50%.  In addition
to doubling the available disk space, RS-6-3 is also more fault
tolerant…a loss of 3 data blocks can be tolerated, where triple replication
can only lose two blocks.&lt;/p&gt;

&lt;p&gt;More storage, better resiliency, so what’s the catch?  One concern is
the time spent calculating the parity blocks.  Unlike replication
, where a client writes a block, and then the DataNodes replicate
the data, an EC HDFS client is responsible for computing the parity and sending that
to the DataNodes.  This increases the CPU and network load on the client.  The CPU
hit can be mitigated by using Intels ISA-L library, but only on CPUs
that support AVX or AVX2 instructions.  (See &lt;a href=&quot;https://www.slideshare.net/HadoopSummit/debunking-the-myths-of-hdfs-erasure-coding-performance&quot;&gt;EC Myths&lt;/a&gt; and &lt;a href=&quot;https://blog.cloudera.com/introduction-to-hdfs-erasure-coding-in-apache-hadoop/&quot;&gt;EC Introduction&lt;/a&gt;
for some interesting claims). In addition, unlike the serial replication I/O path,
the EC I/O path is parallel providing greater throughput. In our testing, sequential writes to 
an EC directory were as much as 3 times faster than a replication directory 
, and reads were up to 2 times faster.&lt;/p&gt;

&lt;p&gt;Another side effect of EC is loss of data locality.  For performance reasons, EC
data blocks are striped, so multiple DataNodes must be contacted to read a single
block of data.  For large sequential reads this is not a
problem, but it can be an issue for small random lookups.  For the latter case,
using RS 6-3 with 64KB stripes mitigates some of the random lookup pain
without compromising sequential read/write performance.&lt;/p&gt;

&lt;h4 id=&quot;important-warning&quot;&gt;Important Warning&lt;/h4&gt;

&lt;p&gt;Before continuing, an important caveat;  the current implementation of EC on Hadoop supports neither hsync
nor hflush.  Both of these operations are silent no-ops (EC &lt;a href=&quot;https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#Limitations&quot;&gt;limitations&lt;/a&gt;).  We discovered this the hard
way when a data center power loss resulted in write-ahead log corruption, which were
stored in an EC directory.  To avoid this problem ensure all 
WAL directories use replication.  It’s probably a good idea to keep the
accumulo namespace replicated as well, but we have no evidence to back up that assertion.  As with all
things, don’t test on production data.&lt;/p&gt;

&lt;h3 id=&quot;ec-performance&quot;&gt;EC Performance&lt;/h3&gt;

&lt;p&gt;To test EC performance, we created a series of clusters on AWS.  Our Accumulo stack consisted of
Hadoop 3.1.1 built with the Intel ISA-L library enabled, Zookeeper 3.4.13, and Accumulo 1.9.3 configured
to work with Hadoop 3 (we did our testing before the official release of Accumulo 2.0). The encoding
policy is set per-directory using the &lt;a href=&quot;https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#Administrative_commands&quot;&gt;hdfs&lt;/a&gt; command-line tool. To set the encoding policy
for an Accumulo table, first find the table ID (for instance using the Accumulo shell’s
“table -l” command), and then from the command line set the policy for the corresponding directory
under /accumulo/tables.  Note that changing the policy on a directory will set the policy for
child directories, but will not change any files contained within.  To change the policy on an existing
Accumulo table, you must first set the encoding policy, and then run a major compaction to rewrite
the RFiles for the table.&lt;/p&gt;

&lt;p&gt;Our first tests were of sequential read and write performance straight to HDFS.  For this test we had
a cluster of 32 HDFS nodes (c5.4xlarge &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/&quot;&gt;AWS&lt;/a&gt; instances), 16 Spark nodes (r5.4xlarge),
3 zookeepers (r5.xlarge), and 1 master (r5.2xlarge).&lt;/p&gt;

&lt;p&gt;The first table below shows the results for writing a 1TB file.  The results are the average of three runs
for each of the directory encodings Reed-Solomon (RS) 6-3 with 64KB stripes, RS 6-3 with 1MB stripes,
RS 10-4 with 1MB stripes, and the default triple replication.  We also varied the number of concurrent
Spark executors, performing tests with 16 executors that did not stress the cluster in any area, and with
128 executors which exhausted our network bandwidth allotment of 5 Gbps. As can be seen, in the 16 executor
environment, we saw greater than a 3X bump in throughput using RS 10-4 with 1MB stripes over triple replication.
At saturation, the speed up was still over 2X, which is in line with the results from &lt;a href=&quot;https://www.slideshare.net/HadoopSummit/debunking-the-myths-of-hdfs-erasure-coding-performance&quot;&gt;EC Myths&lt;/a&gt;. Also of note,
using RS 6-3 with 64KB stripes performed better than the same with 1MB stripes, which is a nice result for Accumulo, 
as we’ll show later.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Encoding&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;16 executors&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;128 executors&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Replication&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.19 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.13 GB/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 64KB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.33 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8.11 GB/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.22 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.93 GB/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 10-4 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.09 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8.34 GB/s&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Our read tests are not as dramatic as those in &lt;a href=&quot;https://www.slideshare.net/HadoopSummit/debunking-the-myths-of-hdfs-erasure-coding-performance&quot;&gt;EC Myths&lt;/a&gt;, but still looking good for EC.  Here we show the
results for reading back the 1TB file created in the write test using 16 Spark executors.  In addition to
the straight read tests, we also performed tests with 2 DataNodes disabled to simulate the performance hit
of failures which require data repair in the foreground.  Finally, we tested the read performance
after a background rebuild of the filesystem.  We did this to see if the foreground rebuild or
the loss of 2 DataNodes was the major contributor to any performance degradation.  As can be seen,
EC read performance is close to 2X faster than replication, even in the face of failures.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Encoding&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;32 nodes&lt;br /&gt;no failures&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;30 nodes&lt;br /&gt;with failures&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;30 nodes&lt;br /&gt;no failures&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Replication&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.95 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.99 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.89 GB/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 64KB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.36 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.27 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.16 GB/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.59 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.47 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.53 GB/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 10-4 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.21 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.08 GB/s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.21 GB/s&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;accumulo-performance-with-ec&quot;&gt;Accumulo Performance with EC&lt;/h3&gt;

&lt;p&gt;While the above results are impressive, they are not representative of how Accumulo uses HDFS.  For starters,
Accumulo sequential I/O is doing far more than just reading or writing files; compression and serialization,
for example, place quite a load upon the tablet server CPUs.  An example to illustrate this is shown below.
The time in minutes to bulk-write 400 million rows to RFiles with 40 Spark executors is listed for both EC
using RS 6-3 with 1MB stripes and triple replication.  The choice of compressor has a much more profound
effect on the write times than the choice of underlying encoding for the directory being written to 
(although without compression EC is much faster than replication).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Compressor&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;RS 6-3 1MB&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Replication&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;File size (GB)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;gz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;21.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;none&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;158.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;snappy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;38.4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Of much more importance to Accumulo performance is read latency. A frequent use case for our group is to obtain a
number of row IDs from an index and then use a BatchScanner to read those individual rows.
In this use case, the time to access a single row is far more important than the raw I/O performance.  To test
Accumulo’s performance with EC for this use case, we did a series of tests against a 10 billion row table,
with each row consisting of 10 columns.  16 Spark executors each performed 10000 queries, where each query
sought 10 random rows.  Thus 16 million individual rows were returned in batches of 10.  For each batch of
10, the time in milliseconds was captured, and theses times were collected in a histogram of 50ms buckets, with
a catch-all bucket for queries that took over 1 second.  For this test we reconfigured our cluster to make use
of c5n.4xlarge nodes featuring must faster networking speeds (15 Gbps sustained vs 5 Gbps for 
c5.4xlarge). Because these nodes are in short supply, we ran with only 16 HDFS nodes (c5n.4xlarge), 
but still had 16 Spark nodes (also c5n.4xlarge).  Zookeeper and master nodes remained the same.&lt;/p&gt;

&lt;p&gt;In the table below, we show the min, max, and average times in milliseconds for each batch of 10 across
four different encoding policies.  The clear winner here is replication, and the clear loser RS 10-4 with 
1MB stripes, but RS 6-3 with 64KB stripes is not looking too bad.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Encoding&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Min&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Avg&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Max&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 10-4 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;105&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;30&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;68&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1297&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 64KB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;43&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1064&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Replication&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;731&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The above results also hold in the event of errors.  The next table shows the same test, but with 2 DataNodes
disabled to simulate failures that require foreground rebuilds.  Again, replication wins, and RS 10-4 1MB
loses, but RS 6-3 64KB remains a viable option.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Encoding&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Min&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Avg&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Max&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 10-4 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;53&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;143&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 1MB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;34&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;113&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1662&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RS 6-3 64KB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;61&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1402&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Replication&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;304&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The images below show a plots of the histograms.  The third plot was generated with 14 HDFS DataNodes, but after
all missing data had been repaired.  Again, this was done to see how much of the performance degradation could be
attributed to missing data, and how much to simply having less computing power available.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/images/blog/201909_ec/ec-latency-16.png&quot; width=&quot;75%&quot; /&gt;&lt;br /&gt;&lt;br /&gt;

&lt;img src=&quot;/images/blog/201909_ec/ec-latency-14e.png&quot; width=&quot;75%&quot; /&gt;&lt;br /&gt;&lt;br /&gt;

&lt;img src=&quot;/images/blog/201909_ec/ec-latency-14.png&quot; width=&quot;75%&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;HDFS with erasure coding has the potential to double your available Accumulo storage, at the cost of a hit in
random seek times, but a potential increase in sequential scan performance. We will be proposing some changes
to Accumulo to make working with EC a bit easier. Our initial thoughts are collected in this 
Accumulo dev list &lt;a href=&quot;https://lists.apache.org/thread.html/4ac5b0f664e15fa120e748892612f1e417b7dee3e1539669d179900c@%3Cdev.accumulo.apache.org%3E&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/09/17/erasure-coding.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/09/17/erasure-coding.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Using S3 as a data store for Accumulo</title>
        <description>&lt;p&gt;Accumulo can store its files in S3, however S3 does not support the needs of
write ahead logs and the Accumulo metadata table. One way to solve this problem
is to store the metadata table and write ahead logs in HDFS and everything else
in S3.  This post shows how to do that using Accumulo 2.0 and Hadoop 3.2.0.
Running on S3 requires a new feature in Accumulo 2.0, that volume choosers are
aware of write ahead logs.&lt;/p&gt;

&lt;h2 id=&quot;hadoop-setup&quot;&gt;Hadoop setup&lt;/h2&gt;

&lt;p&gt;At least the following settings should be added to Hadoop’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;core-site.xml&lt;/code&gt; file on each node in the cluster.&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.s3a.access.key&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;KEY&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.s3a.secret.key&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;SECRET&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;&amp;lt;!-- without this setting Accumulo tservers would have problems when trying to open lots of files --&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.s3a.connection.maximum&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;128&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See &lt;a href=&quot;https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#S3A&quot;&gt;S3A docs&lt;/a&gt;
for more S3A settings.  To get hadoop command to work with s3 set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;export
HADOOP_OPTIONAL_TOOLS=&quot;hadoop-aws&quot;&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hadoop-env.sh&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When trying to use Accumulo with Hadoop’s AWS jar &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-16080&quot;&gt;HADOOP-16080&lt;/a&gt; was
encountered.  The following instructions build a relocated hadoop-aws jar as a
work around.  After building the jar copy it to all nodes in the cluster.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /tmp/haws-reloc
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /tmp/haws-reloc
&lt;span class=&quot;c&quot;&gt;# get the Maven pom file that builds a relocated jar&lt;/span&gt;
wget https://gist.githubusercontent.com/keith-turner/f6dcbd33342732e42695d66509239983/raw/714cb801eb49084e0ceef5c6eb4027334fd51f87/pom.xml
mvn package &lt;span class=&quot;nt&quot;&gt;-Dhadoop&lt;/span&gt;.version&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;your hadoop version&amp;gt;
&lt;span class=&quot;c&quot;&gt;# the new jar will be in target&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ls &lt;/span&gt;target/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;accumulo-setup&quot;&gt;Accumulo setup&lt;/h2&gt;

&lt;p&gt;For each node in the cluster, modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-env.sh&lt;/code&gt; to add S3 jars to the
classpath.  Your versions may differ depending on your Hadoop version,
following versions were included with Hadoop 3.2.0.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_CONF_DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ZOOKEEPER_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/client/*&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:/somedir/hadoop-aws-relocated.3.2.0.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# The following are dependencies needed by by the previous jars and are subject to change&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jaxb-api-2.2.11.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/commons-lang3-3.7jar&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;CLASSPATH
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set the following in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt; and then run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo init&lt;/code&gt;, but don’t start Accumulo.&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;py&quot;&gt;instance.volumes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;name node&amp;gt;/accumulo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After running Accumulo init we need to configure storing write ahead logs in
HDFS.  Set the following in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;py&quot;&gt;instance.volumes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;name node&amp;gt;/accumulo,s3a://&amp;lt;bucket&amp;gt;/accumulo&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.volume.chooser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;org.apache.accumulo.server.fs.PreferredVolumeChooser&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.custom.volume.preferred.default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s3a://&amp;lt;bucket&amp;gt;/accumulo&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.custom.volume.preferred.logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;namenode&amp;gt;/accumulo&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo init --add-volumes&lt;/code&gt; to initialize the S3 volume.  Doing this
in two steps avoids putting any Accumulo metadata files in S3 during init.
Copy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt; to all nodes and start Accumulo.&lt;/p&gt;

&lt;p&gt;Individual tables can be configured to store their files in HDFS by setting the
table property &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;table.custom.volume.preferred&lt;/code&gt;.  This should be set for the
metadata table in case it splits using the following Accumulo shell command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config -t accumulo.metadata -s table.custom.volume.preferred=hdfs://&amp;lt;namenode&amp;gt;/accumulo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;accumulo-example&quot;&gt;Accumulo example&lt;/h2&gt;

&lt;p&gt;The following Accumulo shell session shows an example of writing data to S3 and
reading it back.  It also shows scanning the metadata table to verify the data
is stored in S3.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@muchos&amp;gt; createtable s3test
root@muchos s3test&amp;gt; insert r1 f1 q1 v1
root@muchos s3test&amp;gt; insert r1 f1 q2 v2
root@muchos s3test&amp;gt; flush -w
2019-09-10 19:39:04,695 [shell.Shell] INFO : Flush of table s3test  completed.
root@muchos s3test&amp;gt; scan 
r1 f1:q1 []    v1
r1 f1:q2 []    v2
root@muchos s3test&amp;gt; scan -t accumulo.metadata -c file
2&amp;lt; file:s3a://&amp;lt;bucket&amp;gt;/accumulo/tables/2/default_tablet/F000007b.rf []    234,2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These instructions were only tested a few times and may not result in a stable
system. I have &lt;a href=&quot;https://gist.github.com/keith-turner/149f35f218d10e13227461714012d7bf&quot;&gt;run&lt;/a&gt; a 24hr test with Accumulo and S3.&lt;/p&gt;

&lt;h2 id=&quot;is-s3guard-needed&quot;&gt;Is S3Guard needed?&lt;/h2&gt;

&lt;p&gt;I am not completely certain about this, but I don’t think S3Guard is needed for
regular Accumulo tables.  There are two reasons I think this is so.  First each
Accumulo user tablet stores its list of files in the metadata table using
absolute URIs.  This allows a tablet to have files on multiple DFS instances.
Therefore Accumulo never does a DFS list operation to get a tablets files, it
always uses whats in the metadata table.  Second, Accumulo gives each file a
unique name using a counter stored in Zookeeper and file names are never
reused.&lt;/p&gt;

&lt;p&gt;Things are sligthly different for Accumulo’s metadata.  User tablets store
their file list in the metadata table.  Metadata tablets store their file list
in the root table.  The root table stores its file list in DFS.  Therefore it
would be dangerous to place the root tablet in S3 w/o using S3Guard.  That is
why these instructions place Accumulo metadata in HDFS. &lt;strong&gt;Hopefully&lt;/strong&gt; this
configuration allows the system to be consistent w/o using S3Guard.&lt;/p&gt;

&lt;p&gt;When Accumulo 2.1.0 is released with the changes made by &lt;a href=&quot;https://github.com/apache/accumulo/issues/1313&quot;&gt;#1313&lt;/a&gt; for issue
&lt;a href=&quot;https://github.com/apache/accumulo/issues/936&quot;&gt;#936&lt;/a&gt;, it may be possible to store the metadata table in S3 w/o
S3Gaurd.  If this is the case then only the write ahead logs would need to be
stored in HDFS.&lt;/p&gt;

</description>
        <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/09/10/accumulo-S3-notes.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/09/10/accumulo-S3-notes.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Top 10 Reasons to Upgrade</title>
        <description>&lt;p&gt;Accumulo 2.0 has been in development for quite some time now and is packed with new features, bug
fixes, performance improvements and redesigned components.  All of these changes bring challenges
when upgrading your production cluster so you may be wondering… why should I upgrade?&lt;/p&gt;

&lt;p&gt;My top 10 reasons to upgrade. For all changes see the &lt;a href=&quot;/release/accumulo-2.0.0/&quot;&gt;release notes&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#summaries&quot;&gt;Summaries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#new-bulk-import&quot;&gt;New Bulk Import&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#simplified-scripts-and-config&quot;&gt;Simplified Scripts and Config&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#new-monitor&quot;&gt;New Monitor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#new-apis&quot;&gt;New APIs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#offline-creation&quot;&gt;Offline creation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#search-documentation&quot;&gt;Search Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#new-crypto&quot;&gt;On disk encryption&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#zstandard-compression&quot;&gt;ZStandard Compression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#new-scan-executors&quot;&gt;New Scan Executors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summaries&quot;&gt;Summaries&lt;/h3&gt;

&lt;p&gt;This feature allows detailed stats about Tables to be written directly into Accumulo files (R-Files). 
Summaries can be used to make precise decisions about your data. Once configured, summaries become a 
part of your Tables, so they won’t impact ingest or query performance of your cluster.&lt;/p&gt;

&lt;p&gt;Here are some example use cases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A compaction could automatically run if deletes compose more than 25% of the data&lt;/li&gt;
  &lt;li&gt;An admin could optimize compactions by configuring specific age off of data&lt;/li&gt;
  &lt;li&gt;An admin could analyze R-File summaries for better performance tuning of a cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more info check out the &lt;a href=&quot;/docs/2.x//development/summaries&quot;&gt;summary docs for 2.0&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;new-bulk-import&quot;&gt;New Bulk Import&lt;/h3&gt;

&lt;p&gt;Bulk Ingest was completely redone for 2.0.  Previously, Bulk Ingest relied on expensive inspections of 
R-Files across multiple Tablet Servers. With enough data, an old Bulk Ingest operation could easily 
hold up simpler Table operations and critical compactions of files.&lt;/p&gt;

&lt;p&gt;The new Bulk Ingest gives the user control over the R-File inspection, allows for offline bulk
ingesting and provides performance &lt;a href=&quot;/release/accumulo-2.0.0/#new-bulk-import-api&quot;&gt;improvements&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;simplified-scripts-and-config&quot;&gt;Simplified Scripts and Config&lt;/h2&gt;

&lt;p&gt;Many improvements were done to the scripts and configuration. See Mike’s description of the &lt;a href=&quot;/blog/2016/11/16/simpler-scripts-and-config.html&quot;&gt;improvements.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;new-monitor&quot;&gt;New Monitor&lt;/h2&gt;

&lt;p&gt;The Monitor has been re-written using REST, Javascript and more modern Web Tech.  It is faster, 
cleaner and more maintainable than the previous version. Here is a screen shot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/accumulo-monitor-1.png&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;new-apis&quot;&gt;New APIs&lt;/h2&gt;

&lt;p&gt;Connecting to Accumulo is now easier with a single point of entry for clients. It can now be done with 
a fluent API, 2 imports and using minimal code:&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.accumulo.core.client.Accumulo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.accumulo.core.client.AccumuloClient&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;AccumuloClient&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Accumulo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;newClient&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;instance&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;zk&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;as&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;pass&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;// use the client&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tableOperations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;newTable&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see the client is also closable, which gives developers more control over resources.
See the &lt;a href=&quot;https://static.javadoc.io/org.apache.accumulo/accumulo-core/2.0.1/org/apache/accumulo/core/client/Accumulo.html&quot;&gt;Accumulo entry point javadoc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Key and Mutation have new fluent APIs, which now allow mixing of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;String&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;byte[]&lt;/code&gt; types.&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Key&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newKey&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;builder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;foo&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;family&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bar&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;Mutation&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Mutation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;row0017&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;family&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;001&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;qualifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;v99&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;family&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;002&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;qualifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More examples for &lt;a href=&quot;https://github.com/apache/accumulo/blob/main/core/src/test/java/org/apache/accumulo/core/data/KeyBuilderTest.java&quot;&gt;Key&lt;/a&gt; and &lt;a href=&quot;https://static.javadoc.io/org.apache.accumulo/accumulo-core/2.0.0/org/apache/accumulo/core/data/Mutation.html#at()&quot;&gt;Mutation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;table-creation-options&quot;&gt;Table creation options&lt;/h2&gt;

&lt;p&gt;Tables can now be created with splits, which is much faster than creating a
table and then adding splits.  Tables can also be created in an offline state
now.  The new bulk import API supports offline tables.  This enables the
following method of getting a lot of data into a new table very quickly.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create offline table with splits&lt;/li&gt;
  &lt;li&gt;Bulk import into new offline table&lt;/li&gt;
  &lt;li&gt;Bring table online&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the javadoc for &lt;a href=&quot;https://static.javadoc.io/org.apache.accumulo/accumulo-core/2.0.1/org/apache/accumulo/core/client/admin/NewTableConfiguration.html&quot;&gt;NewTableConfiguration&lt;/a&gt; and search for methods introduced in 2.0.0 for more information.&lt;/p&gt;

&lt;h2 id=&quot;search-documentation&quot;&gt;Search Documentation&lt;/h2&gt;

&lt;p&gt;New ability to quickly search documentation on the website. The user manual was completely redone 
for 2.0. Check it out &lt;a href=&quot;/docs/2.x//getting-started/quickstart&quot;&gt;here&lt;/a&gt;. Users can now quickly &lt;a href=&quot;/search/&quot;&gt;search&lt;/a&gt; the website across all 2.x documentation.&lt;/p&gt;

&lt;h2 id=&quot;new-crypto&quot;&gt;New Crypto&lt;/h2&gt;

&lt;p&gt;On disk encryption was redone to be more secure and flexible. For an in depth description of how Accumulo 
does on disk encryption, see the &lt;a href=&quot;/docs/2.x//security/on-disk-encryption&quot;&gt;user manual&lt;/a&gt;.  NOTE: This is currently an experimental feature.
An experimental feature is considered a work in progress or incomplete and could change.&lt;/p&gt;

&lt;h2 id=&quot;zstandard-compression&quot;&gt;Zstandard compression&lt;/h2&gt;

&lt;p&gt;Support for Zstandard compression was added in 2.0.  It has been measured to perform better than 
gzip (better compression ratio and speed) and snappy (better compression ratio). Checkout Facebook’s &lt;a href=&quot;https://facebook.github.io/zstd/&quot;&gt;github&lt;/a&gt; for Zstandard and
the &lt;a href=&quot;/docs/2.x//configuration/server-properties&quot;&gt;table.file.compress.type&lt;/a&gt; property for configuring Accumulo.&lt;/p&gt;

&lt;h2 id=&quot;new-scan-executors&quot;&gt;New Scan Executors&lt;/h2&gt;

&lt;p&gt;Users now have more control over scans with the new scan executors.  Tables can be configured to utilize these 
powerful new mechanisms using just a few properties, giving user control over things like scan prioritization and 
better cluster resource utilization.&lt;/p&gt;

&lt;p&gt;For example, a cluster has a bunch of long running scans and one really fast scan.  The long running scans will eat up 
a majority of the server resources causing the one really fast scan to be delayed.  Scan executors allow an admin 
to configure the cluster in a way that allows the one fast scan to be prioritized and not have to wait.&lt;/p&gt;

&lt;p&gt;Checkout some examples in the &lt;a href=&quot;/docs/2.x//administration/scan-executors&quot;&gt;user guide&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/08/12/why-upgrade.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/08/12/why-upgrade.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
